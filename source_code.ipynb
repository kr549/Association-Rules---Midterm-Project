{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc6fde6",
   "metadata": {},
   "source": [
    "## Install mlxtend package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5669f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa76b0ed",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7fbc21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n",
    "import warnings\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef5585",
   "metadata": {},
   "source": [
    "## Encoding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342adbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for built-in apriori and fp-growth\n",
    "def encode_data(df):\n",
    "    # Extracting all unique items from Transaction column\n",
    "    all_distinct_items = df['Transactions'].str.split(', ').explode().unique()\n",
    "    encoded_transactions = []\n",
    "\n",
    "    for transaction in df['Transactions']:\n",
    "        transaction_items = transaction.split(', ')\n",
    "        # Create a binary list for each transaction\n",
    "        encoded_transactions.append([1 if item in transaction_items else 0 for item in all_distinct_items])\n",
    "    # Returning the whole encoded dataset as dataframe     \n",
    "    return pd.DataFrame(encoded_transactions, columns=all_distinct_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0fccf8",
   "metadata": {},
   "source": [
    "## Apriori Brute Force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d8bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemset_frequencies(itemset, transactions):\n",
    "    count = 0 # counter for number of transaction\n",
    "    \n",
    "    # Loop to check if the current item is a subset of the transaction, if yes then increment counter\n",
    "    for transaction in transactions:\n",
    "        if set(itemset).issubset(set(transaction)):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def apriori_bruteforce_with_rules(transactions, support_threshold, min_confidence):\n",
    "     # Function to count the frequency of a given itemset in the transactions\n",
    "    def itemset_frequencies(itemset, transactions):\n",
    "        count = 0\n",
    "        for transaction in transactions:\n",
    "            if set(itemset).issubset(set(transaction)):\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    # Function to find all frequent itemsets based on the support threshold\n",
    "    def find_frequent_itemsets(transactions, support_threshold, distinct_items):\n",
    "        frequent_itemsets = []\n",
    "        current_itemsets = []\n",
    "        \n",
    "        print(\">> Frequent 1-itemsets:\\n\")\n",
    "        \n",
    "        # Formatting the output\n",
    "        max_itemset_length = max(len(str(set([item]))) for item in distinct_items) + 5\n",
    "        print(\"-\" * (max_itemset_length + 15))\n",
    "        print(f\"{'Itemset':<{max_itemset_length}} | {'Support':>8}\")\n",
    "        print(\"-\" * (max_itemset_length + 15))\n",
    "\n",
    "        # Loop to count the frequency of each individual item\n",
    "        for item in distinct_items:\n",
    "            count = itemset_frequencies([item], transactions)\n",
    "            if count >= support_threshold:\n",
    "                current_itemsets.append(frozenset([item]))  # Store 1-itemset if frequent\n",
    "                frequent_itemsets.append((frozenset([item]), count))\n",
    "                item_str1 = str(set([item]))\n",
    "                print(f\"{item_str1:<{max_itemset_length}} | {count:>8}\")\n",
    "\n",
    "        print(\"-\" * (max_itemset_length + 15))\n",
    "\n",
    "        k = 2\n",
    "        while current_itemsets:\n",
    "            next_itemsets = []\n",
    "            \n",
    "            # Generate new candidate itemsets by combining current itemsets\n",
    "            for i, itemset1 in enumerate(current_itemsets):\n",
    "                for itemset2 in current_itemsets[i + 1:]:\n",
    "                    union_itemset = itemset1.union(itemset2)\n",
    "                    if len(union_itemset) == k:\n",
    "                        count = itemset_frequencies(union_itemset, transactions)\n",
    "                        if count >= support_threshold and union_itemset not in next_itemsets:\n",
    "                            next_itemsets.append(union_itemset)\n",
    "                            frequent_itemsets.append((union_itemset, count))\n",
    "\n",
    "            if next_itemsets:\n",
    "                max_itemset_length = max(len(str(set(item))) for item in next_itemsets) + 5\n",
    "\n",
    "                print(f\"\\n>> Frequent {k}-itemsets:\\n\")\n",
    "                print(\"-\" * (max_itemset_length + 15))\n",
    "                print(f\"{'Itemset':<{max_itemset_length}} | {'Support':>8}\")\n",
    "                print(\"-\" * (max_itemset_length + 15))\n",
    "\n",
    "                # Display the found frequent itemsets and their counts\n",
    "                for itemset in next_itemsets:\n",
    "                    count = itemset_frequencies(itemset, transactions)\n",
    "                    itemset_str2 = str(set(itemset))\n",
    "                    print(f\"{itemset_str2:<{max_itemset_length}} | {count:>8}\")\n",
    "\n",
    "                print(\"-\" * (max_itemset_length + 15))\n",
    "\n",
    "            current_itemsets = next_itemsets\n",
    "            k += 1\n",
    "\n",
    "        # Displaying final frequent itemsets with their support\n",
    "        print(\"\\n>> Final list of frequent itemsets\")\n",
    "        max_itemset_length = max(len(str(set(itemset))) for itemset, _ in frequent_itemsets) + 5\n",
    "        print(\"-\" * (max_itemset_length + 20))\n",
    "        print(f\"{'Itemset':<{max_itemset_length}} | {'Support':>8}\")  \n",
    "        print(\"-\" * (max_itemset_length + 20))\n",
    "\n",
    "        total_transactions = len(transactions)\n",
    "        for itemset, count in frequent_itemsets:\n",
    "            support = (count / total_transactions) * 100\n",
    "            itemset_str = str(set(itemset))\n",
    "            print(f\"{itemset_str:<{max_itemset_length}} | {support:>6.2f} %\")\n",
    "\n",
    "        print(\"-\" * (max_itemset_length + 20))\n",
    "\n",
    "        return frequent_itemsets\n",
    "\n",
    "    # Function to generate association rules\n",
    "    def generate_association_rules(frequent_itemsets, transactions, min_confidence):\n",
    "        rules = []\n",
    "        total_transactions = len(transactions)\n",
    "\n",
    "        # Function to get all non-empty subsets of an itemset\n",
    "        def get_subsets(itemset):\n",
    "            subsets = []\n",
    "            itemset_list = list(itemset)\n",
    "            for i in range(1, 1 << len(itemset_list)):  # 1 << n gives 2^n combinations\n",
    "                subset = [itemset_list[j] for j in range(len(itemset_list)) if (i & (1 << j))]\n",
    "                if subset and len(subset) < len(itemset_list):\n",
    "                    subsets.append(frozenset(subset))\n",
    "            return subsets\n",
    "\n",
    "        print(\"\\n>> Association Rules:\\n\")\n",
    "        rule_number = 1\n",
    "        \n",
    "        # Generating association rules for each frequent itemset\n",
    "        for itemset, itemset_support_count in frequent_itemsets:\n",
    "            subsets = get_subsets(itemset)\n",
    "            for antecedent in subsets:\n",
    "                consequent = itemset.difference(antecedent)\n",
    "                if consequent:\n",
    "                    antecedent_support_count = itemset_frequencies(antecedent, transactions)\n",
    "                    confidence = itemset_support_count / antecedent_support_count if antecedent_support_count > 0 else 0\n",
    "\n",
    "                    if confidence >= min_confidence:\n",
    "                        support = (itemset_support_count / total_transactions) * 100\n",
    "                        confidence_percent = confidence * 100\n",
    "\n",
    "                        antecedent_str = ', '.join(antecedent)\n",
    "                        consequent_str = ', '.join(consequent)\n",
    "                        \n",
    "                        print(f\"Rule {rule_number}: {antecedent_str} --> {consequent_str} | support = {support:.2f} % | confidence = {confidence_percent:.2f} %\")\n",
    "                        \n",
    "                        rules.append((antecedent_str, consequent_str, support, confidence_percent))\n",
    "                        rule_number += 1\n",
    "\n",
    "        # Handling the case where no rules are generated\n",
    "        if not rules:\n",
    "            print(\"No association rules were generated with the given support and confidence.\")\n",
    "            print(\"Please try again with different support and confidence values.\")\n",
    "        return rules\n",
    "\n",
    "    distinct_items = list(set(item for transaction in transactions for item in transaction))\n",
    "    \n",
    "    frequent_itemsets = find_frequent_itemsets(transactions, support_threshold, distinct_items)\n",
    "    generate_association_rules(frequent_itemsets, transactions, min_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c15592",
   "metadata": {},
   "source": [
    "## Apriori Builtin Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0750af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_builtin(df, min_support, min_confidence):\n",
    "    \n",
    "    df_encoded = encode_data(df)\n",
    "    \n",
    "    # Generate frequent itemsets using the Apriori algorithm\n",
    "    frequent_itemsets = apriori(df_encoded, min_support=min_support/100, use_colnames=True)\n",
    "    \n",
    "    # Generate association rules from the frequent itemsets\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence/100)\n",
    "    \n",
    "    # Handling the case where no rules are generated\n",
    "    if rules.empty:\n",
    "        print(f\"\\nNo association rules were generated with minimum support = {min_support} % and minimum confidence = {min_confidence} %.\")\n",
    "        print(\"Please try again with different support and confidence values.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nAssociation Rules:\\n\")\n",
    "        for i, row in rules.iterrows():\n",
    "            antecedents = ', '.join(list(row['antecedents']))\n",
    "            consequents = ', '.join(list(row['consequents']))\n",
    "            support = row['support'] * 100\n",
    "            confidence = row['confidence'] * 100\n",
    "            print(f\"Rule {i+1}: {antecedents} --> {consequents} | support = {support:.2f} % | confidence = {confidence:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cfafe3",
   "metadata": {},
   "source": [
    "## FP-growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "412204d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_growth(df, min_support, min_confidence):\n",
    "   \n",
    "    df_encoded = encode_data(df)\n",
    "    \n",
    "    # Generate frequent itemsets using the FP-Growth algorithm\n",
    "    frequent_itemsets = fpgrowth(df_encoded, min_support=min_support/100, use_colnames=True)\n",
    "    \n",
    "    # Generate association rules from the frequent itemsets\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence/100)\n",
    "    \n",
    "    # Handling the case where no rules are generated\n",
    "    if rules.empty:\n",
    "        print(f\"\\nNo association rules were generated with minimum support = {min_support} % and minimum confidence = {min_confidence} %.\")\n",
    "        print(\"Please try again with different support and confidence values.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nAssociation Rules:\\n\")\n",
    "        for i, row in rules.iterrows():\n",
    "            antecedents = ', '.join(list(row['antecedents']))\n",
    "            consequents = ', '.join(list(row['consequents']))\n",
    "            support = row['support'] * 100\n",
    "            confidence = row['confidence'] * 100\n",
    "            print(f\"Rule {i+1}: {antecedents} --> {consequents} | support = {support:.2f} % | confidence = {confidence:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa61769",
   "metadata": {},
   "source": [
    "## Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756055bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------\n",
      "                         Midterm Project - Association Rule Mining\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "Choose a retail outlet:\n",
      "1.Amazon\n",
      "2.BestBuy\n",
      "3.KMart\n",
      "4.Nike\n",
      "5.Lidl\n",
      "6.To exit program\n",
      "\n",
      "Enter a number between 1 and 6: 2\n",
      "\n",
      "You have selected the dataset - 2_BestBuy.csv\n",
      "\n",
      "Enter the support value (1 to 100): 50\n",
      "Enter the confidence value (1 to 100): 60\n",
      "\n",
      "\n",
      "\t\t---------- Results of Apriori algorithm using brute force ----------\n",
      "\n",
      ">> Frequent 1-itemsets:\n",
      "\n",
      "-------------------------------------------\n",
      "Itemset                      |  Support\n",
      "-------------------------------------------\n",
      "{'Flash Drive'}              |       13\n",
      "{'Lab Top Case'}             |       14\n",
      "{'Speakers'}                 |       11\n",
      "{'Anti-Virus'}               |       14\n",
      "{'Microsoft Office'}         |       11\n",
      "{'Printer'}                  |       10\n",
      "{'Lab Top'}                  |       12\n",
      "-------------------------------------------\n",
      "\n",
      ">> Frequent 2-itemsets:\n",
      "\n",
      "-------------------------------------------------------\n",
      "Itemset                                  |  Support\n",
      "-------------------------------------------------------\n",
      "{'Anti-Virus', 'Flash Drive'}            |       10\n",
      "{'Microsoft Office', 'Flash Drive'}      |       11\n",
      "{'Printer', 'Flash Drive'}               |       10\n",
      "{'Anti-Virus', 'Lab Top Case'}           |       12\n",
      "{'Lab Top', 'Lab Top Case'}              |       10\n",
      "{'Anti-Virus', 'Lab Top'}                |       10\n",
      "-------------------------------------------------------\n",
      "\n",
      ">> Final list of frequent itemsets\n",
      "------------------------------------------------------------\n",
      "Itemset                                  |  Support\n",
      "------------------------------------------------------------\n",
      "{'Flash Drive'}                          |  65.00 %\n",
      "{'Lab Top Case'}                         |  70.00 %\n",
      "{'Speakers'}                             |  55.00 %\n",
      "{'Anti-Virus'}                           |  70.00 %\n",
      "{'Microsoft Office'}                     |  55.00 %\n",
      "{'Printer'}                              |  50.00 %\n",
      "{'Lab Top'}                              |  60.00 %\n",
      "{'Anti-Virus', 'Flash Drive'}            |  50.00 %\n",
      "{'Microsoft Office', 'Flash Drive'}      |  55.00 %\n",
      "{'Printer', 'Flash Drive'}               |  50.00 %\n",
      "{'Anti-Virus', 'Lab Top Case'}           |  60.00 %\n",
      "{'Lab Top', 'Lab Top Case'}              |  50.00 %\n",
      "{'Anti-Virus', 'Lab Top'}                |  50.00 %\n",
      "------------------------------------------------------------\n",
      "\n",
      ">> Association Rules:\n",
      "\n",
      "Rule 1: Anti-Virus --> Flash Drive | support = 50.00 % | confidence = 71.43 %\n",
      "Rule 2: Flash Drive --> Anti-Virus | support = 50.00 % | confidence = 76.92 %\n",
      "Rule 3: Microsoft Office --> Flash Drive | support = 55.00 % | confidence = 100.00 %\n",
      "Rule 4: Flash Drive --> Microsoft Office | support = 55.00 % | confidence = 84.62 %\n",
      "Rule 5: Printer --> Flash Drive | support = 50.00 % | confidence = 100.00 %\n",
      "Rule 6: Flash Drive --> Printer | support = 50.00 % | confidence = 76.92 %\n",
      "Rule 7: Anti-Virus --> Lab Top Case | support = 60.00 % | confidence = 85.71 %\n",
      "Rule 8: Lab Top Case --> Anti-Virus | support = 60.00 % | confidence = 85.71 %\n",
      "Rule 9: Lab Top --> Lab Top Case | support = 50.00 % | confidence = 83.33 %\n",
      "Rule 10: Lab Top Case --> Lab Top | support = 50.00 % | confidence = 71.43 %\n",
      "Rule 11: Anti-Virus --> Lab Top | support = 50.00 % | confidence = 71.43 %\n",
      "Rule 12: Lab Top --> Anti-Virus | support = 50.00 % | confidence = 83.33 %\n",
      "\n",
      "\n",
      "\t\t---------- Results of Apriori algorithm using builtin function ----------\n",
      "\n",
      "Association Rules:\n",
      "\n",
      "Rule 1: Printer --> Flash Drive | support = 50.00 % | confidence = 100.00 %\n",
      "Rule 2: Flash Drive --> Printer | support = 50.00 % | confidence = 76.92 %\n",
      "Rule 3: Microsoft Office --> Flash Drive | support = 55.00 % | confidence = 100.00 %\n",
      "Rule 4: Flash Drive --> Microsoft Office | support = 55.00 % | confidence = 84.62 %\n",
      "Rule 5: Anti-Virus --> Flash Drive | support = 50.00 % | confidence = 71.43 %\n",
      "Rule 6: Flash Drive --> Anti-Virus | support = 50.00 % | confidence = 76.92 %\n",
      "Rule 7: Anti-Virus --> Lab Top | support = 50.00 % | confidence = 71.43 %\n",
      "Rule 8: Lab Top --> Anti-Virus | support = 50.00 % | confidence = 83.33 %\n",
      "Rule 9: Anti-Virus --> Lab Top Case | support = 60.00 % | confidence = 85.71 %\n",
      "Rule 10: Lab Top Case --> Anti-Virus | support = 60.00 % | confidence = 85.71 %\n",
      "Rule 11: Lab Top --> Lab Top Case | support = 50.00 % | confidence = 83.33 %\n",
      "Rule 12: Lab Top Case --> Lab Top | support = 50.00 % | confidence = 71.43 %\n",
      "\n",
      "\n",
      "\t\t--------------------- Results of FP Growth algorithm ---------------------\n",
      "\n",
      "Association Rules:\n",
      "\n",
      "Rule 1: Anti-Virus --> Lab Top Case | support = 60.00 % | confidence = 85.71 %\n",
      "Rule 2: Lab Top Case --> Anti-Virus | support = 60.00 % | confidence = 85.71 %\n",
      "Rule 3: Anti-Virus --> Flash Drive | support = 50.00 % | confidence = 71.43 %\n",
      "Rule 4: Flash Drive --> Anti-Virus | support = 50.00 % | confidence = 76.92 %\n",
      "Rule 5: Microsoft Office --> Flash Drive | support = 55.00 % | confidence = 100.00 %\n",
      "Rule 6: Flash Drive --> Microsoft Office | support = 55.00 % | confidence = 84.62 %\n",
      "Rule 7: Printer --> Flash Drive | support = 50.00 % | confidence = 100.00 %\n",
      "Rule 8: Flash Drive --> Printer | support = 50.00 % | confidence = 76.92 %\n",
      "Rule 9: Anti-Virus --> Lab Top | support = 50.00 % | confidence = 71.43 %\n",
      "Rule 10: Lab Top --> Anti-Virus | support = 50.00 % | confidence = 83.33 %\n",
      "Rule 11: Lab Top --> Lab Top Case | support = 50.00 % | confidence = 83.33 %\n",
      "Rule 12: Lab Top Case --> Lab Top | support = 50.00 % | confidence = 71.43 %\n",
      "\n",
      "\n",
      "\t\t-------------------------- Computation Time --------------------------\n",
      "\n",
      "Brute force Apriori algorithm     =  0.0027 s\n",
      "Apriori using Built-in function   =  0.011 s\n",
      "FP-Growth using Built-in function =  0.0108 s\n",
      "\n",
      "Do you want to continue (Enter Y/N): N\n",
      "\n",
      "Thank you for running the program! Goodbye!\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def main(): \n",
    "    # Ignore warnings\n",
    "    warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "    \n",
    "    # Mapping of retail outlets to their corresponding CSV files\n",
    "    csv_files = {\n",
    "        1: '1_Amazon.csv',\n",
    "        2: '2_BestBuy.csv',\n",
    "        3: '3_KMart.csv',\n",
    "        4: '4_Nike.csv',\n",
    "        5: '5_Lidl.csv'\n",
    "    }    \n",
    "    \n",
    "    user_choice = \"Y\"\n",
    "    print(\"----------------------------------------------------------------------------------------------\")\n",
    "    print(\"                         Midterm Project - Association Rule Mining\")\n",
    "    print(\"----------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    # Loop to allow the user to select datasets and run the algorithms until they choose to exit\n",
    "    while user_choice.upper()!=\"N\":\n",
    "        try:\n",
    "            print(\"\\nChoose a retail outlet:\")\n",
    "            print(\"1.Amazon\\n2.BestBuy\\n3.KMart\\n4.Nike\\n5.Lidl\\n6.To exit program\")\n",
    "            \n",
    "            # Read user's choice\n",
    "            user_input = int(input(\"\\nEnter a number between 1 and 6: \"))\n",
    "            \n",
    "            if user_input == 6: # To exit\n",
    "                print(\"\\nThank you for running the program! Goodbye!\\n\")\n",
    "                break\n",
    "                \n",
    "            elif user_input in csv_files:\n",
    "                # Reading the corresponding CSV file into a DataFrame\n",
    "                df = pd.read_csv(csv_files[user_input])\n",
    "                print(f\"\\nYou have selected the dataset - {csv_files[user_input]}\\n\")\n",
    "                \n",
    "                # Read minimum support and confidence from user \n",
    "                support = float(input(\"Enter the support value (1 to 100): \"))\n",
    "                confidence = float(input(\"Enter the confidence value (1 to 100): \"))\n",
    "                \n",
    "                # Compute total transactions and support threshold\n",
    "                total_transactions = len(df)\n",
    "                support_threshold = (support / 100) * total_transactions\n",
    "\n",
    "                # Extract distinct items from the transactions for frequency calculation\n",
    "                all_items = df['Transactions'].str.split(', ').explode()\n",
    "                distinct_items = all_items.unique()\n",
    "                item_counts = all_items.value_counts()\n",
    "                transactions = df['Transactions'].str.split(', ').tolist()\n",
    "                \n",
    "                # Frequent 1-itemsets\n",
    "                frequent_items = item_counts[item_counts >= support_threshold]  \n",
    "                \n",
    "                # Calling the three functions and computing their execution time \n",
    "                \n",
    "                # Apriori - Brute Force\n",
    "                print(\"\\n\\n\\t\\t---------- Results of Apriori algorithm using brute force ----------\\n\")\n",
    "                t1 = time.perf_counter()\n",
    "                apriori_bruteforce_with_rules(transactions, support_threshold, confidence/100)\n",
    "                apriori1_time = time.perf_counter() - t1\n",
    "                               \n",
    "                \n",
    "                # Apriori - Builtin Function\n",
    "                print(\"\\n\\n\\t\\t---------- Results of Apriori algorithm using builtin function ----------\")\n",
    "                t2 = time.perf_counter()\n",
    "                apriori_builtin(df, support, confidence)\n",
    "                apriori2_time = time.perf_counter() - t2\n",
    "                \n",
    "                \n",
    "                # FP Tree\n",
    "                print(\"\\n\\n\\t\\t--------------------- Results of FP Growth algorithm ---------------------\")       \n",
    "                t3 = time.perf_counter()\n",
    "                fp_growth(df, support, confidence)\n",
    "                fpg_time = time.perf_counter() - t3\n",
    "\n",
    "                \n",
    "                # Printing computational time\n",
    "                print(\"\\n\\n\\t\\t-------------------------- Computation Time --------------------------\\n\")\n",
    "                print(\"Brute force Apriori algorithm     = \", round(apriori1_time, 4), \"s\")\n",
    "                print(\"Apriori using Built-in function   = \", round(apriori2_time, 4), \"s\")\n",
    "                print(\"FP-Growth using Built-in function = \", round(fpg_time, 4), \"s\")\n",
    "                \n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please try again.\")\n",
    "        \n",
    "        user_choice = input(\"\\nDo you want to continue (Enter Y/N): \")  \n",
    "    print(\"\\nThank you for running the program! Goodbye!\\n\")\n",
    "    print(\"----------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e5e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
